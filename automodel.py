# -*- coding: utf-8 -*-
"""AutoModelForMultipleChoice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17isj5cHmNuw2SQ_8NLt7wp54LUIeubOf
"""





from huggingface_hub import notebook_login
notebook_login()

import os, sys, random, ast

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import json
import datasets
from datasets import Dataset, DatasetDict

import torch

from dataclasses import dataclass

from transformers import AutoTokenizer, AutoModelForMultipleChoice
from transformers import Trainer, TrainingArguments, set_seed
from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy

from typing import Optional, Union

# Function to read our JSON file
def read_json(folder_path):
    dataset = []
    k = 0
    print(os.listdir(folder_path))
    for item in os.listdir(folder_path):
        print(k)
        k = k+1
        try:
          with open(os.path.join(folder_path, item), 'r', encoding="utf8", errors='ignore') as file:
              data = json.load(file)
              dataset.extend(data)
        except Exception as e:
          print(e)
    return dataset

# Load and preprocess training data
train_dataset_path = '/Users/devaduttsanka/desktop/NLP_PROJECT-MAIN/data/training_dataset'

train_data = read_json(train_dataset_path)

# Load and preprocess testing data
test_dataset_path = '/Users/devaduttsanka/desktop/NLP_PROJECT-MAIN/data/validation_dataset'

test_data = read_json(test_dataset_path)

train_df = pd.DataFrame(train_data)

test_df = pd.DataFrame(test_data)

print(train_df.head())

print(test_df.head())

traindata = pd.DataFrame()

answer0,answer1,answer2,answer3 = [],[],[],[]
labels = []
story = []
question = []
k = 0
for item in train_df.iterrows():
    if len(item[1]['Answer Choices'])==4:
        answer0.append(item[1]['Answer Choices'][0])
        answer1.append(item[1]['Answer Choices'][1])
        answer2.append(item[1]['Answer Choices'][2])
        answer3.append(item[1]['Answer Choices'][3])
        choice = item[1]['Answer Choices']
        answer = item[1]['Answer']
        try:
          idx = choice.index(answer)
        except Exception as e:
          try:
            stri = 'ABCD'
            temp = stri.index(answer.upper())
            idx = temp
          except Exception as e:
            k+=1
        labels.append(idx)
        question.append(item[1]['Question'])
        story.append(item[1]['Story'])
print(k)
traindata['Story'] = story
traindata['Question'] = question
traindata['answer0'] = answer0
traindata['answer1'] = answer1
traindata['answer2'] = answer2
traindata['answer3'] = answer3
traindata['labels'] = labels

traindata.sample(5)

testdata = pd.DataFrame()

answer0,answer1,answer2,answer3 = [],[],[],[]
labels = []
story = []
question = []
k = 0
for item in test_df.iterrows():
    if len(item[1]['Answer Choices'])==4:
        answer0.append(item[1]['Answer Choices'][0])
        answer1.append(item[1]['Answer Choices'][1])
        answer2.append(item[1]['Answer Choices'][2])
        answer3.append(item[1]['Answer Choices'][3])
        choice = item[1]['Answer Choices']
        answer = item[1]['Answer']
        try:
          idx = choice.index(answer)
        except Exception as e:
          try:
            stri = 'ABCD'
            temp = stri.index(answer.upper())
            idx = temp
          except Exception as e:
            k+=1
        labels.append(idx)
        question.append(item[1]['Question'])
        story.append(item[1]['Story'])
print(k)
testdata['Story'] = story
testdata['Question'] = question
testdata['answer0'] = answer0
testdata['answer1'] = answer1
testdata['answer2'] = answer2
testdata['answer3'] = answer3
testdata['labels'] = labels

print(testdata.sample(5))

# Split the DataFrame into train_df and val_df
train_df = traindata
test_df = testdata

# Reset the index for both DataFrames (optional)
train_df = train_df.reset_index(drop=True)
test_df = test_df.reset_index(drop=True)

print(len(train_df), len(test_df))

train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

ds = DatasetDict({
    'train' : train_dataset,
    'test' : test_dataset,
})

print("Training Dataset Shape:", ds['train'].shape)
print("Testing Dataset Shape:", ds['test'].shape)

print(ds['test'].features)

def show_one_example(sample):
    print(f"Context: {sample['Story']} {sample['Question']}")
    print(f" 0 - {sample['answer0']}")
    print(f" 1 - {sample['answer1']}")
    print(f" 2 - {sample['answer2']}")
    print(f" 3 - {sample['answer3']}")
    print(f"\nGround Truth: option {sample['labels']}")

show_one_example(ds['train'][2])

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)

ending_names = ["answer0", "answer1", "answer2", "answer3"]

def preprocess_function(samples):
    first_sentences = [[context] * len(ending_names) for context in samples['Story']]

    question_headers = samples['Question']
    second_sentences = [[f"{header} {samples[end][i]}" for end in ending_names] for i, header in enumerate(question_headers)]

    first_sentences = sum(first_sentences, [])
    second_sentences = sum(second_sentences, [])

    tokenized_samples = tokenizer(first_sentences, second_sentences, truncation=True)

    return {k: [v[i:i + len(ending_names)] for i in range(0, len(v), len(ending_names))] for k, v in tokenized_samples.items()}

encoded_ds = ds.map(preprocess_function, batched=True)

encoded_ds['train'].features

encoded_ds['test'].features

model = AutoModelForMultipleChoice.from_pretrained('bert-base-uncased')

@dataclass
class DataCollatorForMultipleChoice:
    """
    Data collator that will dynamically pad inputs
    for the multiple choices received.
    """

    tokenizer: PreTrainedTokenizerBase
    padding: Union[bool, str, PaddingStrategy] = True
    max_length: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None

    def __call__(self, features):
        label_name = "label" if "label" in features[0].keys() else "labels"
        labels = [feature.pop(label_name) for feature in features]
        batch_size = len(features)
        num_choices = len(features[0]["input_ids"])
        flattened_features = [[{k: v[i] for k, v in feature.items()}
                               for i in range(num_choices)] for feature in features]
        flattened_features = sum(flattened_features, [])

        batch = self.tokenizer.pad(
            flattened_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors="pt",
        )

        # Unflatten
        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}
        # Add back labels
        batch["labels"] = torch.tensor(labels, dtype=torch.int64)
        return batch

def compute_metrics(predictions):
    preds, labels = predictions
    preds = np.argmax(preds, axis=1)
    return {"accuracy": (preds == labels).astype(np.float32).mean().item()}

MODEL_CKPT = 'bert-base-uncased'
MODEL_NAME = MODEL_CKPT.split("/")[-1] + "-MCQ_Task"

STRATEGY = "epoch"
LEARNING_RATE = 5e-5
BATCH_SIZE = 16
NUM_OF_EPOCHS = 5
MAX_LENGTH = 512
WEIGHT_DECAY = 0.01
REPORTS_TO = "tensorboard"

set_seed = 42

args = TrainingArguments(
    output_dir='bert-base-uncased-MCQ_Task',
    evaluation_strategy="epoch",
    per_device_train_batch_size=6,
    per_device_eval_batch_size=6,
    num_train_epochs=5,
    weight_decay=0.01,
    logging_first_step=True,
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=encoded_ds["train"],
    eval_dataset=encoded_ds["test"],
    tokenizer=tokenizer,
    data_collator=DataCollatorForMultipleChoice(tokenizer),
    compute_metrics=compute_metrics,
)

train_results = trainer.train()

metrics = trainer.evaluate()

print(metrics)

